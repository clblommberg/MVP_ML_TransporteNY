{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de datos\n",
    "## Importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import pandas as pd\n",
    "from datetime import date, time\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os import listdir\n",
    "# from os.path import isfile, join\n",
    "#fhvhv_tripdata_2022-01.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cargar_archivos_parquet(directorio, archivos_a_considerar):\n",
    "    # Crear un diccionario para almacenar los DataFrames de Polars\n",
    "    diccionario_dataframes = {}\n",
    "\n",
    "    # Iterar sobre los archivos Parquet a considerar\n",
    "    for archivo in archivos_a_considerar:\n",
    "        # Construir la ruta completa al archivo\n",
    "        ruta_archivo = os.path.join(directorio, archivo)\n",
    "        \n",
    "        # Leer el archivo Parquet en un DataFrame de Polars\n",
    "        df = pl.read_parquet(ruta_archivo)\n",
    "        \n",
    "        # Eliminar filas con valores nulos\n",
    "        df = df.drop_nulls()\n",
    "        \n",
    "        # Obtener el mes del archivo\n",
    "        mes = archivo.split('_')[2].split('-')[1].split('.')[0]  # Extraer el mes del nombre del archivo\n",
    "        \n",
    "        # Agregar el DataFrame al diccionario utilizando el mes como clave\n",
    "        diccionario_dataframes[mes] = df\n",
    "\n",
    "    return diccionario_dataframes\n",
    "\n",
    "# Ruta al directorio que contiene los archivos Parquet\n",
    "directorio = \"..\\\\datasets\\\\raw\\\\\"\n",
    "\n",
    "# Número de archivos a considerar\n",
    "num_archivos = 2\n",
    "\n",
    "# Generar dinámicamente los nombres de archivo para los archivos a considerar\n",
    "archivos_por_mes = [f\"fhvhv_tripdata_2023-{str(i).zfill(2)}.parquet\" for i in range(1, num_archivos + 1)]\n",
    "\n",
    "# Cargar solo los archivos especificados\n",
    "diccionario_dataframes = cargar_archivos_parquet(directorio, archivos_por_mes)\n",
    "\n",
    "# Ahora el diccionario 'diccionario_dataframes' contiene un DataFrame por mes para los archivos Parquet especificados,\n",
    "# donde se han eliminado las filas con valores nulos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información del DataFrame '01':\n",
      "Cantidad de registros : 13587039 Cantidad de las Columnas : 24\n",
      "Schema\n",
      "OrderedDict([('hvfhs_license_num', String), ('dispatching_base_num', String), ('originating_base_num', String), ('request_datetime', Datetime(time_unit='ns', time_zone=None)), ('on_scene_datetime', Datetime(time_unit='ns', time_zone=None)), ('pickup_datetime', Datetime(time_unit='ns', time_zone=None)), ('dropoff_datetime', Datetime(time_unit='ns', time_zone=None)), ('PULocationID', Int64), ('DOLocationID', Int64), ('trip_miles', Float64), ('trip_time', Int64), ('base_passenger_fare', Float64), ('tolls', Float64), ('bcf', Float64), ('sales_tax', Float64), ('congestion_surcharge', Float64), ('airport_fee', Float64), ('tips', Float64), ('driver_pay', Float64), ('shared_request_flag', String), ('shared_match_flag', String), ('access_a_ride_flag', String), ('wav_request_flag', String), ('wav_match_flag', String)])\n",
      "\n",
      "\n",
      "Información del DataFrame '02':\n",
      "Cantidad de registros : 13287430 Cantidad de las Columnas : 24\n",
      "Schema\n",
      "OrderedDict([('hvfhs_license_num', String), ('dispatching_base_num', String), ('originating_base_num', String), ('request_datetime', Datetime(time_unit='ns', time_zone=None)), ('on_scene_datetime', Datetime(time_unit='ns', time_zone=None)), ('pickup_datetime', Datetime(time_unit='ns', time_zone=None)), ('dropoff_datetime', Datetime(time_unit='ns', time_zone=None)), ('PULocationID', Int32), ('DOLocationID', Int32), ('trip_miles', Float64), ('trip_time', Int64), ('base_passenger_fare', Float64), ('tolls', Float64), ('bcf', Float64), ('sales_tax', Float64), ('congestion_surcharge', Float64), ('airport_fee', Float64), ('tips', Float64), ('driver_pay', Float64), ('shared_request_flag', String), ('shared_match_flag', String), ('access_a_ride_flag', String), ('wav_request_flag', String), ('wav_match_flag', String)])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterar sobre el diccionario de dataframes\n",
    "for nombre, df in diccionario_dataframes.items():\n",
    "    print(f\"Información del DataFrame '{nombre}':\")\n",
    "    print(\"Cantidad de registros :\", len(df), \"Cantidad de las Columnas :\", len(df.columns))\n",
    "    print(\"Schema\")\n",
    "    print(df.schema)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las columnas con diferencias son:\n",
      "        columna   tipo      _merge\n",
      "0  DOLocationID  Int32  right_only\n",
      "1  DOLocationID  Int64   left_only\n",
      "2  PULocationID  Int32  right_only\n",
      "3  PULocationID  Int64   left_only\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Esquema del DataFrame '01'\n",
    "esquema_01 = pd.DataFrame({\n",
    "    'columna': ['hvfhs_license_num', 'dispatching_base_num', 'originating_base_num', 'request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID', 'trip_miles', 'trip_time', 'base_passenger_fare', 'tolls', 'bcf', 'sales_tax', 'congestion_surcharge', 'airport_fee', 'tips', 'driver_pay', 'shared_request_flag', 'shared_match_flag', 'access_a_ride_flag', 'wav_request_flag', 'wav_match_flag'],\n",
    "    'tipo': ['String', 'String', 'String', 'Datetime', 'Datetime', 'Datetime', 'Datetime', 'Int64', 'Int64', 'Float64', 'Int64', 'Float64', 'Float64', 'Float64', 'Float64', 'Float64', 'Float64', 'Float64', 'Float64', 'String', 'String', 'String', 'String', 'String']\n",
    "})\n",
    "\n",
    "# Esquema del DataFrame '02'\n",
    "esquema_02 = pd.DataFrame({\n",
    "    'columna': ['hvfhs_license_num', 'dispatching_base_num', 'originating_base_num', 'request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID', 'trip_miles', 'trip_time', 'base_passenger_fare', 'tolls', 'bcf', 'sales_tax', 'congestion_surcharge', 'airport_fee', 'tips', 'driver_pay', 'shared_request_flag', 'shared_match_flag', 'access_a_ride_flag', 'wav_request_flag', 'wav_match_flag'],\n",
    "    'tipo': ['String', 'String', 'String', 'Datetime', 'Datetime', 'Datetime', 'Datetime', 'Int32', 'Int32', 'Float64', 'Int64', 'Float64', 'Float64', 'Float64', 'Float64', 'Float64', 'Float64', 'Float64', 'Float64', 'String', 'String', 'String', 'String', 'String']\n",
    "})\n",
    "\n",
    "# Encontrar las diferencias entre los dos esquemas\n",
    "diferencias = esquema_01.merge(esquema_02, indicator=True, how='outer').loc[lambda x: x['_merge'] != 'both']\n",
    "\n",
    "# Imprimir las diferencias\n",
    "print(\"Las columnas con diferencias son:\")\n",
    "print(diferencias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('hvfhs_license_num', String), ('dispatching_base_num', String), ('originating_base_num', String), ('request_datetime', Datetime(time_unit='ns', time_zone=None)), ('on_scene_datetime', Datetime(time_unit='ns', time_zone=None)), ('pickup_datetime', Datetime(time_unit='ns', time_zone=None)), ('dropoff_datetime', Datetime(time_unit='ns', time_zone=None)), ('PULocationID', Int32), ('DOLocationID', Int32), ('trip_miles', Float64), ('trip_time', Int64), ('base_passenger_fare', Float64), ('tolls', Float64), ('bcf', Float64), ('sales_tax', Float64), ('congestion_surcharge', Float64), ('airport_fee', Float64), ('tips', Float64), ('driver_pay', Float64), ('shared_request_flag', String), ('shared_match_flag', String), ('access_a_ride_flag', String), ('wav_request_flag', String), ('wav_match_flag', String)])\n"
     ]
    }
   ],
   "source": [
    "# Supongamos que diccionario_dataframes es tu diccionario que contiene los DataFrames\n",
    "# Reemplaza diccionario_dataframes con el nombre de tu diccionario\n",
    "\n",
    "# Tomar el primer DataFrame del diccionario\n",
    "primer_df = next(iter(diccionario_dataframes.values()))\n",
    "\n",
    "# Definir un nuevo esquema con los tipos de datos y nombres de columna deseados\n",
    "nuevo_esquema = {\n",
    "    \"hvfhs_license_num\": pl.String,\n",
    "    \"dispatching_base_num\": pl.String,\n",
    "    \"originating_base_num\": pl.String,\n",
    "    \"request_datetime\": pl.Datetime(time_unit='ns', time_zone=None),\n",
    "    \"on_scene_datetime\": pl.Datetime(time_unit='ns', time_zone=None),\n",
    "    \"pickup_datetime\": pl.Datetime(time_unit='ns', time_zone=None),\n",
    "    \"dropoff_datetime\": pl.Datetime(time_unit='ns', time_zone=None),\n",
    "    \"PULocationID\": pl.Int32,\n",
    "    \"DOLocationID\": pl.Int32,\n",
    "    \"trip_miles\": pl.Float64,\n",
    "    \"trip_time\": pl.Int64,\n",
    "    \"base_passenger_fare\": pl.Float64,\n",
    "    \"tolls\": pl.Float64,\n",
    "    \"bcf\": pl.Float64,\n",
    "    \"sales_tax\": pl.Float64,\n",
    "    \"congestion_surcharge\": pl.Float64,\n",
    "    \"airport_fee\": pl.Float64,\n",
    "    \"tips\": pl.Float64,\n",
    "    \"driver_pay\": pl.Float64,\n",
    "    \"shared_request_flag\": pl.String,\n",
    "    \"shared_match_flag\": pl.String,\n",
    "    \"access_a_ride_flag\": pl.String,\n",
    "    \"wav_request_flag\": pl.String,\n",
    "    \"wav_match_flag\": pl.String,\n",
    "}\n",
    "\n",
    "# Renombrar las columnas y cambiar los tipos de datos del DataFrame\n",
    "df = primer_df.select([\n",
    "    pl.col(col).cast(dtype).alias(col) for col, dtype in nuevo_esquema.items()\n",
    "])\n",
    "\n",
    "# Cambiar el nombre de la columna \"airport_fee\" a \"Airport_fee\"\n",
    "#df = df.with_columns(primer_df['airport_fee'].alias('Airport_fee'))\n",
    "#df = df.drop('airport_fee')\n",
    "# Reemplazar el DataFrame original en el diccionario con el DataFrame modificado\n",
    "nombre_clave = next(iter(diccionario_dataframes))  # Obtener la clave del primer DataFrame\n",
    "diccionario_dataframes[nombre_clave] = df\n",
    "\n",
    "# Verificar el resultado\n",
    "print(diccionario_dataframes[nombre_clave].schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre el diccionario de dataframes\n",
    "# for nombre, df in diccionario_dataframes.items():\n",
    "#     print(f\"Información del DataFrame '{nombre}':\")\n",
    "#     print(\"Cantidad de registros :\", len(df), \"Cantidad de las Columnas :\", len(df.columns))\n",
    "#     print(\"Schema\")\n",
    "#     print(df.schema)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora que todos los DataFrames tienen las mismas columnas, puedes proceder a concatenarlos.\n",
    "dataframes = list(diccionario_dataframes.values())\n",
    "\n",
    "# Concatenar verticalmente todos los DataFrames\n",
    "df_concatenado = pl.concat(dataframes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26874469, 24)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenado.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hvfhs_license_num', String),\n",
       "             ('dispatching_base_num', String),\n",
       "             ('originating_base_num', String),\n",
       "             ('request_datetime', Datetime(time_unit='ns', time_zone=None)),\n",
       "             ('on_scene_datetime', Datetime(time_unit='ns', time_zone=None)),\n",
       "             ('pickup_datetime', Datetime(time_unit='ns', time_zone=None)),\n",
       "             ('dropoff_datetime', Datetime(time_unit='ns', time_zone=None)),\n",
       "             ('PULocationID', Int32),\n",
       "             ('DOLocationID', Int32),\n",
       "             ('trip_miles', Float64),\n",
       "             ('trip_time', Int64),\n",
       "             ('base_passenger_fare', Float64),\n",
       "             ('tolls', Float64),\n",
       "             ('bcf', Float64),\n",
       "             ('sales_tax', Float64),\n",
       "             ('congestion_surcharge', Float64),\n",
       "             ('airport_fee', Float64),\n",
       "             ('tips', Float64),\n",
       "             ('driver_pay', Float64),\n",
       "             ('shared_request_flag', String),\n",
       "             ('shared_match_flag', String),\n",
       "             ('access_a_ride_flag', String),\n",
       "             ('wav_request_flag', String),\n",
       "             ('wav_match_flag', String)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenado.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 24)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>hvfhs_license_num</th><th>dispatching_base_num</th><th>originating_base_num</th><th>request_datetime</th><th>on_scene_datetime</th><th>pickup_datetime</th><th>dropoff_datetime</th><th>PULocationID</th><th>DOLocationID</th><th>trip_miles</th><th>trip_time</th><th>base_passenger_fare</th><th>tolls</th><th>bcf</th><th>sales_tax</th><th>congestion_surcharge</th><th>airport_fee</th><th>tips</th><th>driver_pay</th><th>shared_request_flag</th><th>shared_match_flag</th><th>access_a_ride_flag</th><th>wav_request_flag</th><th>wav_match_flag</th></tr><tr><td>str</td><td>str</td><td>str</td><td>datetime[ns]</td><td>datetime[ns]</td><td>datetime[ns]</td><td>datetime[ns]</td><td>i32</td><td>i32</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;HV0003&quot;</td><td>&quot;B03404&quot;</td><td>&quot;B03404&quot;</td><td>2023-01-01 00:18:06</td><td>2023-01-01 00:19:24</td><td>2023-01-01 00:19:38</td><td>2023-01-01 00:48:07</td><td>48</td><td>68</td><td>0.94</td><td>1709</td><td>25.95</td><td>0.0</td><td>0.78</td><td>2.3</td><td>2.75</td><td>0.0</td><td>5.22</td><td>27.83</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>&quot; &quot;</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td></tr><tr><td>&quot;HV0003&quot;</td><td>&quot;B03404&quot;</td><td>&quot;B03404&quot;</td><td>2023-01-01 00:48:42</td><td>2023-01-01 00:56:20</td><td>2023-01-01 00:58:39</td><td>2023-01-01 01:33:08</td><td>246</td><td>163</td><td>2.78</td><td>2069</td><td>60.14</td><td>0.0</td><td>1.8</td><td>5.34</td><td>2.75</td><td>0.0</td><td>0.0</td><td>50.15</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>&quot; &quot;</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td></tr><tr><td>&quot;HV0003&quot;</td><td>&quot;B03404&quot;</td><td>&quot;B03404&quot;</td><td>2023-01-01 00:15:35</td><td>2023-01-01 00:20:14</td><td>2023-01-01 00:20:27</td><td>2023-01-01 00:37:54</td><td>9</td><td>129</td><td>8.81</td><td>1047</td><td>24.37</td><td>0.0</td><td>0.73</td><td>2.16</td><td>0.0</td><td>0.0</td><td>0.0</td><td>20.22</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>&quot; &quot;</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td></tr><tr><td>&quot;HV0003&quot;</td><td>&quot;B03404&quot;</td><td>&quot;B03404&quot;</td><td>2023-01-01 00:35:24</td><td>2023-01-01 00:39:30</td><td>2023-01-01 00:41:05</td><td>2023-01-01 00:48:16</td><td>129</td><td>129</td><td>0.67</td><td>431</td><td>13.8</td><td>0.0</td><td>0.41</td><td>1.22</td><td>0.0</td><td>0.0</td><td>0.0</td><td>7.9</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>&quot; &quot;</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td></tr><tr><td>&quot;HV0003&quot;</td><td>&quot;B03404&quot;</td><td>&quot;B03404&quot;</td><td>2023-01-01 00:43:15</td><td>2023-01-01 00:51:10</td><td>2023-01-01 00:52:47</td><td>2023-01-01 01:04:51</td><td>129</td><td>92</td><td>4.38</td><td>724</td><td>20.49</td><td>0.0</td><td>0.61</td><td>1.82</td><td>0.0</td><td>0.0</td><td>0.0</td><td>16.48</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>&quot; &quot;</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 24)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ hvfhs_lic ┆ dispatchi ┆ originati ┆ request_d ┆ … ┆ shared_ma ┆ access_a_ ┆ wav_reque ┆ wav_matc │\n",
       "│ ense_num  ┆ ng_base_n ┆ ng_base_n ┆ atetime   ┆   ┆ tch_flag  ┆ ride_flag ┆ st_flag   ┆ h_flag   │\n",
       "│ ---       ┆ um        ┆ um        ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ str       ┆ ---       ┆ ---       ┆ datetime[ ┆   ┆ str       ┆ str       ┆ str       ┆ str      │\n",
       "│           ┆ str       ┆ str       ┆ ns]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ HV0003    ┆ B03404    ┆ B03404    ┆ 2023-01-0 ┆ … ┆ N         ┆           ┆ N         ┆ N        │\n",
       "│           ┆           ┆           ┆ 1         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆ 00:18:06  ┆   ┆           ┆           ┆           ┆          │\n",
       "│ HV0003    ┆ B03404    ┆ B03404    ┆ 2023-01-0 ┆ … ┆ N         ┆           ┆ N         ┆ N        │\n",
       "│           ┆           ┆           ┆ 1         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆ 00:48:42  ┆   ┆           ┆           ┆           ┆          │\n",
       "│ HV0003    ┆ B03404    ┆ B03404    ┆ 2023-01-0 ┆ … ┆ N         ┆           ┆ N         ┆ N        │\n",
       "│           ┆           ┆           ┆ 1         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆ 00:15:35  ┆   ┆           ┆           ┆           ┆          │\n",
       "│ HV0003    ┆ B03404    ┆ B03404    ┆ 2023-01-0 ┆ … ┆ N         ┆           ┆ N         ┆ N        │\n",
       "│           ┆           ┆           ┆ 1         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆ 00:35:24  ┆   ┆           ┆           ┆           ┆          │\n",
       "│ HV0003    ┆ B03404    ┆ B03404    ┆ 2023-01-0 ┆ … ┆ N         ┆           ┆ N         ┆ N        │\n",
       "│           ┆           ┆           ┆ 1         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆ 00:43:15  ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenado.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tpep_pickup_datetime|tpep_dropoff_datetime / fechas  a segundos y fechas unicamente\n",
    "# trip_distance tratar las filas con distanca cero \n",
    "# RatecodeID| 6 datos adminitidos  (99) valores faltantes\n",
    "# payment_type 1 y 2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26874469, 24)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenado.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nyc = df_concatenado.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hvfhs_license_num', String),\n",
       "             ('dispatching_base_num', String),\n",
       "             ('originating_base_num', String),\n",
       "             ('request_datetime', Datetime(time_unit='ns', time_zone=None)),\n",
       "             ('on_scene_datetime', Datetime(time_unit='ns', time_zone=None)),\n",
       "             ('pickup_datetime', Datetime(time_unit='ns', time_zone=None)),\n",
       "             ('dropoff_datetime', Datetime(time_unit='ns', time_zone=None)),\n",
       "             ('PULocationID', Int32),\n",
       "             ('DOLocationID', Int32),\n",
       "             ('trip_miles', Float64),\n",
       "             ('trip_time', Int64),\n",
       "             ('base_passenger_fare', Float64),\n",
       "             ('tolls', Float64),\n",
       "             ('bcf', Float64),\n",
       "             ('sales_tax', Float64),\n",
       "             ('congestion_surcharge', Float64),\n",
       "             ('airport_fee', Float64),\n",
       "             ('tips', Float64),\n",
       "             ('driver_pay', Float64),\n",
       "             ('shared_request_flag', String),\n",
       "             ('shared_match_flag', String),\n",
       "             ('access_a_ride_flag', String),\n",
       "             ('wav_request_flag', String),\n",
       "             ('wav_match_flag', String)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nyc.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Preprocessing\n",
    "### Preprocesamiento Variables `Temporales` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 24)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>hvfhs_license_num</th><th>dispatching_base_num</th><th>originating_base_num</th><th>request_datetime</th><th>on_scene_datetime</th><th>pickup_datetime</th><th>dropoff_datetime</th><th>PULocationID</th><th>DOLocationID</th><th>trip_miles</th><th>trip_time</th><th>base_passenger_fare</th><th>tolls</th><th>bcf</th><th>sales_tax</th><th>congestion_surcharge</th><th>airport_fee</th><th>tips</th><th>driver_pay</th><th>shared_request_flag</th><th>shared_match_flag</th><th>access_a_ride_flag</th><th>wav_request_flag</th><th>wav_match_flag</th></tr><tr><td>str</td><td>str</td><td>str</td><td>datetime[ns]</td><td>datetime[ns]</td><td>datetime[ns]</td><td>datetime[ns]</td><td>i32</td><td>i32</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;HV0003&quot;</td><td>&quot;B03404&quot;</td><td>&quot;B03404&quot;</td><td>2023-01-01 00:18:06</td><td>2023-01-01 00:19:24</td><td>2023-01-01 00:19:38</td><td>2023-01-01 00:48:07</td><td>48</td><td>68</td><td>0.94</td><td>1709</td><td>25.95</td><td>0.0</td><td>0.78</td><td>2.3</td><td>2.75</td><td>0.0</td><td>5.22</td><td>27.83</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>&quot; &quot;</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td></tr><tr><td>&quot;HV0003&quot;</td><td>&quot;B03404&quot;</td><td>&quot;B03404&quot;</td><td>2023-01-01 00:48:42</td><td>2023-01-01 00:56:20</td><td>2023-01-01 00:58:39</td><td>2023-01-01 01:33:08</td><td>246</td><td>163</td><td>2.78</td><td>2069</td><td>60.14</td><td>0.0</td><td>1.8</td><td>5.34</td><td>2.75</td><td>0.0</td><td>0.0</td><td>50.15</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>&quot; &quot;</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td></tr><tr><td>&quot;HV0003&quot;</td><td>&quot;B03404&quot;</td><td>&quot;B03404&quot;</td><td>2023-01-01 00:15:35</td><td>2023-01-01 00:20:14</td><td>2023-01-01 00:20:27</td><td>2023-01-01 00:37:54</td><td>9</td><td>129</td><td>8.81</td><td>1047</td><td>24.37</td><td>0.0</td><td>0.73</td><td>2.16</td><td>0.0</td><td>0.0</td><td>0.0</td><td>20.22</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>&quot; &quot;</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td></tr><tr><td>&quot;HV0003&quot;</td><td>&quot;B03404&quot;</td><td>&quot;B03404&quot;</td><td>2023-01-01 00:35:24</td><td>2023-01-01 00:39:30</td><td>2023-01-01 00:41:05</td><td>2023-01-01 00:48:16</td><td>129</td><td>129</td><td>0.67</td><td>431</td><td>13.8</td><td>0.0</td><td>0.41</td><td>1.22</td><td>0.0</td><td>0.0</td><td>0.0</td><td>7.9</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>&quot; &quot;</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td></tr><tr><td>&quot;HV0003&quot;</td><td>&quot;B03404&quot;</td><td>&quot;B03404&quot;</td><td>2023-01-01 00:43:15</td><td>2023-01-01 00:51:10</td><td>2023-01-01 00:52:47</td><td>2023-01-01 01:04:51</td><td>129</td><td>92</td><td>4.38</td><td>724</td><td>20.49</td><td>0.0</td><td>0.61</td><td>1.82</td><td>0.0</td><td>0.0</td><td>0.0</td><td>16.48</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>&quot; &quot;</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 24)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ hvfhs_lic ┆ dispatchi ┆ originati ┆ request_d ┆ … ┆ shared_ma ┆ access_a_ ┆ wav_reque ┆ wav_matc │\n",
       "│ ense_num  ┆ ng_base_n ┆ ng_base_n ┆ atetime   ┆   ┆ tch_flag  ┆ ride_flag ┆ st_flag   ┆ h_flag   │\n",
       "│ ---       ┆ um        ┆ um        ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ str       ┆ ---       ┆ ---       ┆ datetime[ ┆   ┆ str       ┆ str       ┆ str       ┆ str      │\n",
       "│           ┆ str       ┆ str       ┆ ns]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ HV0003    ┆ B03404    ┆ B03404    ┆ 2023-01-0 ┆ … ┆ N         ┆           ┆ N         ┆ N        │\n",
       "│           ┆           ┆           ┆ 1         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆ 00:18:06  ┆   ┆           ┆           ┆           ┆          │\n",
       "│ HV0003    ┆ B03404    ┆ B03404    ┆ 2023-01-0 ┆ … ┆ N         ┆           ┆ N         ┆ N        │\n",
       "│           ┆           ┆           ┆ 1         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆ 00:48:42  ┆   ┆           ┆           ┆           ┆          │\n",
       "│ HV0003    ┆ B03404    ┆ B03404    ┆ 2023-01-0 ┆ … ┆ N         ┆           ┆ N         ┆ N        │\n",
       "│           ┆           ┆           ┆ 1         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆ 00:15:35  ┆   ┆           ┆           ┆           ┆          │\n",
       "│ HV0003    ┆ B03404    ┆ B03404    ┆ 2023-01-0 ┆ … ┆ N         ┆           ┆ N         ┆ N        │\n",
       "│           ┆           ┆           ┆ 1         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆ 00:35:24  ┆   ┆           ┆           ┆           ┆          │\n",
       "│ HV0003    ┆ B03404    ┆ B03404    ┆ 2023-01-0 ┆ … ┆ N         ┆           ┆ N         ┆ N        │\n",
       "│           ┆           ┆           ┆ 1         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆ 00:43:15  ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nyc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir las columnas de fechas a tipo DateTime en Polars\n",
    "df_nyc = df_concatenado.with_columns(\n",
    "    pl.col('pickup_datetime').dt.date().alias('tpep_pickup_date'),\n",
    "    pl.col('dropoff_datetime').dt.date().alias('tpep_dropoff_date')\n",
    ")\n",
    "\n",
    "# Calcular la duración del viaje en segundos\n",
    "df_nyc = df_nyc.with_columns(\n",
    "    (pl.col('dropoff_datetime') - pl.col('pickup_datetime')).dt.total_seconds().alias('viaje_segundos')\n",
    ")\n",
    "# Eliminar las columnas originales de fecha y hora\n",
    "df_nyc = df_nyc.drop(['pickup_datetime', 'dropoff_datetime'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir las columnas de fechas a tipo DateTime en Polars\n",
    "df_nyc = df_nyc.with_columns(\n",
    "    pl.col('request_datetime').dt.date().alias('trequest_datetime'),\n",
    "    pl.col('on_scene_datetime').dt.date().alias('tscene_datetime')\n",
    ")\n",
    "\n",
    "# Calcular la duración del viaje en segundos\n",
    "df_nyc = df_nyc.with_columns(\n",
    "    (pl.col('on_scene_datetime') - pl.col('request_datetime')).dt.total_seconds().alias('espera_segundos')\n",
    ")\n",
    "# Eliminar las columnas originales de fecha y hora\n",
    "df_nyc = df_nyc.drop(['request_datetime', 'on_scene_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 26)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>hvfhs_license_num</th><th>dispatching_base_num</th><th>originating_base_num</th><th>PULocationID</th><th>DOLocationID</th><th>trip_miles</th><th>trip_time</th><th>base_passenger_fare</th><th>tolls</th><th>bcf</th><th>sales_tax</th><th>congestion_surcharge</th><th>airport_fee</th><th>tips</th><th>driver_pay</th><th>shared_request_flag</th><th>shared_match_flag</th><th>access_a_ride_flag</th><th>wav_request_flag</th><th>wav_match_flag</th><th>tpep_pickup_date</th><th>tpep_dropoff_date</th><th>viaje_segundos</th><th>trequest_datetime</th><th>tscene_datetime</th><th>espera_segundos</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i32</td><td>i32</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>date</td><td>date</td><td>i64</td><td>date</td><td>date</td><td>i64</td></tr></thead><tbody><tr><td>&quot;HV0003&quot;</td><td>&quot;B03404&quot;</td><td>&quot;B03404&quot;</td><td>48</td><td>68</td><td>0.94</td><td>1709</td><td>25.95</td><td>0.0</td><td>0.78</td><td>2.3</td><td>2.75</td><td>0.0</td><td>5.22</td><td>27.83</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>&quot; &quot;</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>2023-01-01</td><td>2023-01-01</td><td>1709</td><td>2023-01-01</td><td>2023-01-01</td><td>78</td></tr><tr><td>&quot;HV0003&quot;</td><td>&quot;B03404&quot;</td><td>&quot;B03404&quot;</td><td>246</td><td>163</td><td>2.78</td><td>2069</td><td>60.14</td><td>0.0</td><td>1.8</td><td>5.34</td><td>2.75</td><td>0.0</td><td>0.0</td><td>50.15</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>&quot; &quot;</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>2023-01-01</td><td>2023-01-01</td><td>2069</td><td>2023-01-01</td><td>2023-01-01</td><td>458</td></tr><tr><td>&quot;HV0003&quot;</td><td>&quot;B03404&quot;</td><td>&quot;B03404&quot;</td><td>9</td><td>129</td><td>8.81</td><td>1047</td><td>24.37</td><td>0.0</td><td>0.73</td><td>2.16</td><td>0.0</td><td>0.0</td><td>0.0</td><td>20.22</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>&quot; &quot;</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>2023-01-01</td><td>2023-01-01</td><td>1047</td><td>2023-01-01</td><td>2023-01-01</td><td>279</td></tr><tr><td>&quot;HV0003&quot;</td><td>&quot;B03404&quot;</td><td>&quot;B03404&quot;</td><td>129</td><td>129</td><td>0.67</td><td>431</td><td>13.8</td><td>0.0</td><td>0.41</td><td>1.22</td><td>0.0</td><td>0.0</td><td>0.0</td><td>7.9</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>&quot; &quot;</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>2023-01-01</td><td>2023-01-01</td><td>431</td><td>2023-01-01</td><td>2023-01-01</td><td>246</td></tr><tr><td>&quot;HV0003&quot;</td><td>&quot;B03404&quot;</td><td>&quot;B03404&quot;</td><td>129</td><td>92</td><td>4.38</td><td>724</td><td>20.49</td><td>0.0</td><td>0.61</td><td>1.82</td><td>0.0</td><td>0.0</td><td>0.0</td><td>16.48</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>&quot; &quot;</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>2023-01-01</td><td>2023-01-01</td><td>724</td><td>2023-01-01</td><td>2023-01-01</td><td>475</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 26)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ hvfhs_lic ┆ dispatchi ┆ originati ┆ PULocatio ┆ … ┆ viaje_seg ┆ trequest_ ┆ tscene_da ┆ espera_s │\n",
       "│ ense_num  ┆ ng_base_n ┆ ng_base_n ┆ nID       ┆   ┆ undos     ┆ datetime  ┆ tetime    ┆ egundos  │\n",
       "│ ---       ┆ um        ┆ um        ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ str       ┆ ---       ┆ ---       ┆ i32       ┆   ┆ i64       ┆ date      ┆ date      ┆ i64      │\n",
       "│           ┆ str       ┆ str       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ HV0003    ┆ B03404    ┆ B03404    ┆ 48        ┆ … ┆ 1709      ┆ 2023-01-0 ┆ 2023-01-0 ┆ 78       │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ 1         ┆ 1         ┆          │\n",
       "│ HV0003    ┆ B03404    ┆ B03404    ┆ 246       ┆ … ┆ 2069      ┆ 2023-01-0 ┆ 2023-01-0 ┆ 458      │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ 1         ┆ 1         ┆          │\n",
       "│ HV0003    ┆ B03404    ┆ B03404    ┆ 9         ┆ … ┆ 1047      ┆ 2023-01-0 ┆ 2023-01-0 ┆ 279      │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ 1         ┆ 1         ┆          │\n",
       "│ HV0003    ┆ B03404    ┆ B03404    ┆ 129       ┆ … ┆ 431       ┆ 2023-01-0 ┆ 2023-01-0 ┆ 246      │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ 1         ┆ 1         ┆          │\n",
       "│ HV0003    ┆ B03404    ┆ B03404    ┆ 129       ┆ … ┆ 724       ┆ 2023-01-0 ┆ 2023-01-0 ┆ 475      │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ 1         ┆ 1         ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nyc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hvfhs_license_num',\n",
       " 'dispatching_base_num',\n",
       " 'originating_base_num',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'trip_miles',\n",
       " 'trip_time',\n",
       " 'base_passenger_fare',\n",
       " 'tolls',\n",
       " 'bcf',\n",
       " 'sales_tax',\n",
       " 'congestion_surcharge',\n",
       " 'airport_fee',\n",
       " 'tips',\n",
       " 'driver_pay',\n",
       " 'shared_request_flag',\n",
       " 'shared_match_flag',\n",
       " 'access_a_ride_flag',\n",
       " 'wav_request_flag',\n",
       " 'wav_match_flag',\n",
       " 'tpep_pickup_date',\n",
       " 'tpep_dropoff_date',\n",
       " 'viaje_segundos',\n",
       " 'trequest_datetime',\n",
       " 'tscene_datetime',\n",
       " 'espera_segundos']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nyc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26874469, 26)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nyc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar las columnas originales de fecha y hora\n",
    "df_nyc = df_nyc.drop(['dispatching_base_num', 'originating_base_num', 'access_a_ride_flag', 'wav_request_flag', 'wav_match_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26874469, 21)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nyc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el DataFrame de Polars a un objeto Arrow\n",
    "arrow_table = df_nyc.to_arrow()\n",
    "\n",
    "# Guardar el objeto Arrow en un archivo Arrow\n",
    "#pq.write_table(arrow_table, 'temperature.arrow')\n",
    "#pq.write_table(arrow_table, '../datasets/processed/df_yellow.arrow')\n",
    "pq.write_table(arrow_table, '../datasets/processed/df_ffvh.arrow', compression='zstd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hvfhs_license_num', String),\n",
       "             ('PULocationID', Int32),\n",
       "             ('DOLocationID', Int32),\n",
       "             ('trip_miles', Float64),\n",
       "             ('trip_time', Int64),\n",
       "             ('base_passenger_fare', Float64),\n",
       "             ('tolls', Float64),\n",
       "             ('bcf', Float64),\n",
       "             ('sales_tax', Float64),\n",
       "             ('congestion_surcharge', Float64),\n",
       "             ('airport_fee', Float64),\n",
       "             ('tips', Float64),\n",
       "             ('driver_pay', Float64),\n",
       "             ('shared_request_flag', String),\n",
       "             ('shared_match_flag', String),\n",
       "             ('tpep_pickup_date', Date),\n",
       "             ('tpep_dropoff_date', Date),\n",
       "             ('viaje_segundos', Int64),\n",
       "             ('trequest_datetime', Date),\n",
       "             ('tscene_datetime', Date),\n",
       "             ('espera_segundos', Int64)])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_nyc.describe()\n",
    "df_nyc.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimo: -1694897908\n",
      "Maximo: 601751\n",
      "Media: 997.5984043618433\n",
      "Mediana: 752.0\n",
      "Desviación estándar: 278647.64816650894\n"
     ]
    }
   ],
   "source": [
    "# Calcular la media de la columna 'tviaje_segundos'\n",
    "mean_seconds = df_nyc['viaje_segundos'].mean()\n",
    "\n",
    "# Calcular la mediana de la columna 'tviaje_segundos'\n",
    "median_seconds = df_nyc['viaje_segundos'].median()\n",
    "\n",
    "# Calcular la desviación estándar de la columna 'tviaje_segundos'\n",
    "std_seconds = df_nyc['viaje_segundos'].std()\n",
    "\n",
    "# Obtener el valor mínimo de la columna 'viaje_segundos'\n",
    "min_seconds = df_nyc['viaje_segundos'].min()\n",
    "\n",
    "# Obtener el valor máximo de la columna 'viaje_segundos'\n",
    "max_seconds = df_nyc['viaje_segundos'].max()\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Minimo:\", min_seconds)\n",
    "print(\"Maximo:\", max_seconds)\n",
    "print(\"Media:\", mean_seconds)\n",
    "print(\"Mediana:\", median_seconds)\n",
    "print(\"Desviación estándar:\", std_seconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de registros con valores negativos en 'viaje_segundos': 819\n"
     ]
    }
   ],
   "source": [
    "# Filtrar los registros con valores negativos en la columna 'viaje_segundos'\n",
    "negativos = df_nyc.filter(pl.col('viaje_segundos') < 0.0)\n",
    "\n",
    "# Obtener la cantidad de registros con valores negativos\n",
    "cantidad_negativos = negativos.height\n",
    "print(\"Cantidad de registros con valores negativos en 'viaje_segundos':\", cantidad_negativos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Eliminar los registros con valores negativos en la columna 'viaje_segundos'\n",
    "#df_nyc = df_nyc.drop(pl.col('viaje_segundos') < 0.0)\n",
    "# Filtrar las filas que cumplen la condición\n",
    "df_yellow = df_nyc.filter(pl.col('viaje_segundos') >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pl.scan_ipc(\"temperature.arrow\")\n",
    "# create a SQL context, registering the frame as a table\n",
    "#sql = pl.SQLContext(my_table=df)\n",
    "# create a SQL query to execute\n",
    "# Leer el archivo Arrow\n",
    "arrow_table = pq.read_table('../datasets/processed/temperature.arrow')\n",
    "\n",
    "# Convertir el objeto Arrow a un DataFrame de Polars\n",
    "df_polars = pl.from_arrow(arrow_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (17_520, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>event_date</th><th>temperature_2m</th></tr><tr><td>datetime[ns, UTC]</td><td>f32</td></tr></thead><tbody><tr><td>2022-01-01 00:00:00 UTC</td><td>8.346</td></tr><tr><td>2022-01-01 01:00:00 UTC</td><td>9.146</td></tr><tr><td>2022-01-01 02:00:00 UTC</td><td>7.996</td></tr><tr><td>2022-01-01 03:00:00 UTC</td><td>8.046</td></tr><tr><td>2022-01-01 04:00:00 UTC</td><td>7.646</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2023-12-31 19:00:00 UTC</td><td>6.396</td></tr><tr><td>2023-12-31 20:00:00 UTC</td><td>6.296</td></tr><tr><td>2023-12-31 21:00:00 UTC</td><td>5.796</td></tr><tr><td>2023-12-31 22:00:00 UTC</td><td>4.396</td></tr><tr><td>2023-12-31 23:00:00 UTC</td><td>3.446</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (17_520, 2)\n",
       "┌─────────────────────────┬────────────────┐\n",
       "│ event_date              ┆ temperature_2m │\n",
       "│ ---                     ┆ ---            │\n",
       "│ datetime[ns, UTC]       ┆ f32            │\n",
       "╞═════════════════════════╪════════════════╡\n",
       "│ 2022-01-01 00:00:00 UTC ┆ 8.346          │\n",
       "│ 2022-01-01 01:00:00 UTC ┆ 9.146          │\n",
       "│ 2022-01-01 02:00:00 UTC ┆ 7.996          │\n",
       "│ 2022-01-01 03:00:00 UTC ┆ 8.046          │\n",
       "│ 2022-01-01 04:00:00 UTC ┆ 7.646          │\n",
       "│ …                       ┆ …              │\n",
       "│ 2023-12-31 19:00:00 UTC ┆ 6.396          │\n",
       "│ 2023-12-31 20:00:00 UTC ┆ 6.296          │\n",
       "│ 2023-12-31 21:00:00 UTC ┆ 5.796          │\n",
       "│ 2023-12-31 22:00:00 UTC ┆ 4.396          │\n",
       "│ 2023-12-31 23:00:00 UTC ┆ 3.446          │\n",
       "└─────────────────────────┴────────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_polars "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "0  1  4\n",
      "1  2  5\n",
      "2  3  6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "# Ejemplo de DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6]\n",
    "})\n",
    "\n",
    "# Convertir el DataFrame de Pandas a un DataFrame de Polars\n",
    "df_polars = pl.from_pandas(df)\n",
    "\n",
    "# Convertir el DataFrame de Polars a un objeto Arrow\n",
    "arrow_table = df_polars.to_arrow()\n",
    "\n",
    "# Guardar el objeto Arrow en un archivo Arrow\n",
    "pq.write_table(arrow_table, 'data.arrow')\n",
    "\n",
    "# Leer el archivo Arrow utilizando pl.scan_pyarrow_dataset()\n",
    "dset = ds.dataset(\"data.arrow\", format=\"ipc\")  \n",
    "df_loaded = (\n",
    "    pl.scan_pyarrow_dataset(dset)\n",
    "    .to_pandas()  # Convertir a DataFrame de Pandas para mostrarlo\n",
    ")\n",
    "\n",
    "# Mostrar el DataFrame cargado\n",
    "print(df_loaded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Leer el archivo Arrow\n",
    "arrow_table = pq.read_table('../datasets/processed/temperature.arrow')\n",
    "\n",
    "# Convertir el objeto Arrow a un DataFrame de Polars\n",
    "df_polars = pl.from_arrow(arrow_table)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (17_520, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>event_date</th><th>temperature_2m</th></tr><tr><td>datetime[ns, UTC]</td><td>f32</td></tr></thead><tbody><tr><td>2022-01-01 00:00:00 UTC</td><td>8.346</td></tr><tr><td>2022-01-01 01:00:00 UTC</td><td>9.146</td></tr><tr><td>2022-01-01 02:00:00 UTC</td><td>7.996</td></tr><tr><td>2022-01-01 03:00:00 UTC</td><td>8.046</td></tr><tr><td>2022-01-01 04:00:00 UTC</td><td>7.646</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2023-12-31 19:00:00 UTC</td><td>6.396</td></tr><tr><td>2023-12-31 20:00:00 UTC</td><td>6.296</td></tr><tr><td>2023-12-31 21:00:00 UTC</td><td>5.796</td></tr><tr><td>2023-12-31 22:00:00 UTC</td><td>4.396</td></tr><tr><td>2023-12-31 23:00:00 UTC</td><td>3.446</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (17_520, 2)\n",
       "┌─────────────────────────┬────────────────┐\n",
       "│ event_date              ┆ temperature_2m │\n",
       "│ ---                     ┆ ---            │\n",
       "│ datetime[ns, UTC]       ┆ f32            │\n",
       "╞═════════════════════════╪════════════════╡\n",
       "│ 2022-01-01 00:00:00 UTC ┆ 8.346          │\n",
       "│ 2022-01-01 01:00:00 UTC ┆ 9.146          │\n",
       "│ 2022-01-01 02:00:00 UTC ┆ 7.996          │\n",
       "│ 2022-01-01 03:00:00 UTC ┆ 8.046          │\n",
       "│ 2022-01-01 04:00:00 UTC ┆ 7.646          │\n",
       "│ …                       ┆ …              │\n",
       "│ 2023-12-31 19:00:00 UTC ┆ 6.396          │\n",
       "│ 2023-12-31 20:00:00 UTC ┆ 6.296          │\n",
       "│ 2023-12-31 21:00:00 UTC ┆ 5.796          │\n",
       "│ 2023-12-31 22:00:00 UTC ┆ 4.396          │\n",
       "│ 2023-12-31 23:00:00 UTC ┆ 3.446          │\n",
       "└─────────────────────────┴────────────────┘"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    event_date  temperature_2m\n",
      "0    2022-01-01 00:00:00+00:00           8.346\n",
      "1    2022-01-01 01:00:00+00:00           9.146\n",
      "2    2022-01-01 02:00:00+00:00           7.996\n",
      "3    2022-01-01 03:00:00+00:00           8.046\n",
      "4    2022-01-01 04:00:00+00:00           7.646\n",
      "...                        ...             ...\n",
      "7169 2023-12-31 19:00:00+00:00           6.396\n",
      "7170 2023-12-31 20:00:00+00:00           6.296\n",
      "7171 2023-12-31 21:00:00+00:00           5.796\n",
      "7172 2023-12-31 22:00:00+00:00           4.396\n",
      "7173 2023-12-31 23:00:00+00:00           3.446\n",
      "\n",
      "[7174 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Crear un contexto SQL y registrar el DataFrame como una tabla\n",
    "sql = pl.SQLContext(frames={\"df_polars\": df_polars})\n",
    "\n",
    "result = sql.execute(\n",
    "    \"SELECT * FROM df_polars WHERE temperature_2m < 10\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Convertir el resultado a un DataFrame de Pandas para mostrarlo\n",
    "result_df = result.collect().to_pandas()\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
